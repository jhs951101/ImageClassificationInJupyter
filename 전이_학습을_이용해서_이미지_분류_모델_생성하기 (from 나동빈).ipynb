{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8085e751d5f4e3a87cdc0620a61a4c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9b0d26bc1aaa4160a4aec1a1e27c4275",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9698aaad26054bdfbda124144e2567d2",
              "IPY_MODEL_987213c93c4d4485858f78a3f13284db"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "9b0d26bc1aaa4160a4aec1a1e27c4275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "9698aaad26054bdfbda124144e2567d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e53f02803df0495b97382c8a6b95c8a2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9b3baf046ac49b1b35ed6e65abbcfd8"
          },
          "model_module_version": "1.5.0"
        },
        "987213c93c4d4485858f78a3f13284db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e3a2338952c5458285dff75ff9f1de1f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [03:48&lt;00:00, 382kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41fedcb2a0b04a719e2f7197047fd2fc"
          },
          "model_module_version": "1.5.0"
        },
        "e53f02803df0495b97382c8a6b95c8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "a9b3baf046ac49b1b35ed6e65abbcfd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "e3a2338952c5458285dff75ff9f1de1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "41fedcb2a0b04a719e2f7197047fd2fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAU00Uaacc_k"
      },
      "source": [
        "<h1>전이 학습(Transfer Learning)을 이용해서 이미지 분류 모델 생성하기</h1>\n",
        "\n",
        "1. 왼쪽에서 폴더란을 열고 [마우스 우클릭]-[새 폴더]를 클릭해 폴더를 생성하고, 폴더 안에 [마우스 우클릭]-[업로드]를 클릭해 이미지들을 업로드 합니다.\n",
        "\n",
        "2. 상단 메뉴 [런타임]-[모두 실행]을 클릭해 이 프로그램을 실행시킵니다. 학습을 시켜야 하므로 5분 이상은 소요될 겁니다 ㅠㅠ\n",
        "\n",
        "3. 맨 밑의 출력 결과에 'Running on http://1a2b3c4d5e.ngrok.io' 라고 나오는 부분이 있습니다. 위와 같이 나오는 링크를 TLECommander.py 파일에서 맨 윗줄에 선언되어 있는 link 변수에 대입합니다. copy 변수에 True를 대입하면 복사, False를 대입하면 이동합니다.\n",
        "\n",
        "4. TLECommander.py 파일과 같이 있는 images 폴더에 분류시킬 이미지들을 넣어놓고 TLECommander.py 프로그램을 실행시키면 이미지들을 분류해줍니다. 분류 결과는 result 폴더에서 확인할 수 있습니다.\n",
        "\n",
        "** TLECommander.py 파일<br>\n",
        "라이브러리: os, shutil<br>\n",
        "버전: Python 3.6.5<br>\n",
        "운영체제: MacOS Big Sur 11.3<br><br>\n",
        "\n",
        "\n",
        "<출처><br>\n",
        "3분 만에 만드는 인공지능 서비스 개발 강좌: 마동석/김종국/이병헌 분류기, 나동빈,<br>\n",
        "https://www.youtube.com/watch?v=Lu93Ah2h9XA&t=6s &<br>\n",
        "https://github.com/ndb796/CNN-based-Celebrity-Classification-AI-Service-Using-Transfer-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y9g65dez3Ny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "448ec193-b2f9-4d26-a130-f1dd1083bcdc"
      },
      "source": [
        "import os\n",
        "\n",
        "querys = []\n",
        "imgext = ['.jpeg', '.jpg', '.png', '.gif', '.bmp', '.tif']\n",
        "\n",
        "for folder in os.listdir('./'):\n",
        "    if os.path.isdir(folder):\n",
        "        for file in os.listdir('./' + folder):\n",
        "            if os.path.isfile('./' + folder + '/' + file):\n",
        "                found = False\n",
        "                for i in range(0, len(imgext), 1):\n",
        "                    if imgext[i] in file.lower():\n",
        "                        querys.append(folder)\n",
        "                        found = True\n",
        "                        break\n",
        "                \n",
        "                if found:\n",
        "                    break\n",
        "\n",
        "print('분류 기준: ', querys)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "분류 기준:  ['kotori', 'honoka']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km0yMQoGgGRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e8abec-9fc3-4c9c-d56a-89164699c831"
      },
      "source": [
        "!apt install fonts-nanum -y\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=10)\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.font_manager._rebuild()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB]\n",
            "Fetched 9,604 kB in 1s (10.1 MB/s)\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3az4in0AR5R"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "directory_list = [\n",
        "    './custom_dataset/train/',\n",
        "    './custom_dataset/test/',\n",
        "]\n",
        "\n",
        "for directory in directory_list:\n",
        "    if not os.path.isdir(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "def dataset_split(query, train_cnt):\n",
        "    for directory in directory_list:\n",
        "        if not os.path.isdir(directory + '/' + query):\n",
        "            os.makedirs(directory + '/' + query)\n",
        "    cnt = 0\n",
        "    for file_name in os.listdir(query):\n",
        "        for i in range(0, len(imgext), 1):\n",
        "            if imgext[i] in file_name.lower():\n",
        "                if cnt < train_cnt:\n",
        "                    print(f'[Train Dataset] {file_name}')\n",
        "                    shutil.copy(query + '/' + file_name, './custom_dataset/train/' + query + '/' + file_name)\n",
        "                    # shutil.rmtree('')\n",
        "                else:\n",
        "                    print(f'[Test Dataset] {file_name}')\n",
        "                    shutil.copy(query + '/' + file_name, './custom_dataset/test/' + query + '/' + file_name)\n",
        "\n",
        "            cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm1ucFykGhwk"
      },
      "source": [
        "for i in range(0, len(querys), 1):\n",
        "  dataset_split(querys[i], 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqBBf2rcagrR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# cuda:0 or cpu\n",
        "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4f-9uSvclTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810bb2b1-3b9d-48c5-8acf-25d8a3201b97"
      },
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=4, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=4, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZVEGzhUco_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fcd6c06-ccf3-4c93-d139-4ce25a48f22c"
      },
      "source": [
        "def imshow(input, title):\n",
        "    input = input.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    input = std * input + mean\n",
        "    input = np.clip(input, 0, 1)\n",
        "    plt.imshow(input)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "iterator = iter(train_dataloader)\n",
        "\n",
        "inputs, classes = next(iterator)\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jQ431m3j4Y-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "f8085e751d5f4e3a87cdc0620a61a4c8",
            "9b0d26bc1aaa4160a4aec1a1e27c4275",
            "9698aaad26054bdfbda124144e2567d2",
            "987213c93c4d4485858f78a3f13284db",
            "e53f02803df0495b97382c8a6b95c8a2",
            "a9b3baf046ac49b1b35ed6e65abbcfd8",
            "e3a2338952c5458285dff75ff9f1de1f",
            "41fedcb2a0b04a719e2f7197047fd2fc"
          ]
        },
        "outputId": "78bc24e9-50dc-44ce-f908-8b72ca7b1a50"
      },
      "source": [
        "model = models.resnet34(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 3)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8085e751d5f4e3a87cdc0620a61a4c8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3ojcj83iHnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e37452e-8e49-4d90-ee12-49e46e49ebb9"
      },
      "source": [
        "num_epochs = 50\n",
        "model.train()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#0 Loss: 1.3960 Acc: 50.0000% Time: 5.1018s\n",
            "#1 Loss: 0.6517 Acc: 70.0000% Time: 10.0222s\n",
            "#2 Loss: 0.4214 Acc: 80.0000% Time: 14.9244s\n",
            "#3 Loss: 0.4021 Acc: 70.0000% Time: 19.8216s\n",
            "#4 Loss: 0.0562 Acc: 100.0000% Time: 24.7395s\n",
            "#5 Loss: 0.3263 Acc: 80.0000% Time: 29.6939s\n",
            "#6 Loss: 0.0157 Acc: 100.0000% Time: 34.6536s\n",
            "#7 Loss: 0.2913 Acc: 80.0000% Time: 39.6346s\n",
            "#8 Loss: 0.0069 Acc: 100.0000% Time: 44.5593s\n",
            "#9 Loss: 0.1613 Acc: 90.0000% Time: 49.5957s\n",
            "#10 Loss: 0.4282 Acc: 80.0000% Time: 54.5889s\n",
            "#11 Loss: 0.2078 Acc: 90.0000% Time: 59.5464s\n",
            "#12 Loss: 0.0087 Acc: 100.0000% Time: 64.4391s\n",
            "#13 Loss: 0.6516 Acc: 80.0000% Time: 69.3707s\n",
            "#14 Loss: 1.2408 Acc: 70.0000% Time: 74.3473s\n",
            "#15 Loss: 0.0064 Acc: 100.0000% Time: 79.2445s\n",
            "#16 Loss: 0.4728 Acc: 80.0000% Time: 84.1723s\n",
            "#17 Loss: 0.0069 Acc: 100.0000% Time: 89.0527s\n",
            "#18 Loss: 0.0521 Acc: 100.0000% Time: 93.8761s\n",
            "#19 Loss: 0.3142 Acc: 90.0000% Time: 98.7412s\n",
            "#20 Loss: 0.1628 Acc: 90.0000% Time: 103.5386s\n",
            "#21 Loss: 0.0345 Acc: 100.0000% Time: 108.3646s\n",
            "#22 Loss: 0.7107 Acc: 70.0000% Time: 113.1710s\n",
            "#23 Loss: 0.0031 Acc: 100.0000% Time: 118.0284s\n",
            "#24 Loss: 0.1481 Acc: 90.0000% Time: 122.8334s\n",
            "#25 Loss: 0.0038 Acc: 100.0000% Time: 127.6453s\n",
            "#26 Loss: 0.0084 Acc: 100.0000% Time: 132.5638s\n",
            "#27 Loss: 0.0150 Acc: 100.0000% Time: 137.5169s\n",
            "#28 Loss: 0.0121 Acc: 100.0000% Time: 142.3565s\n",
            "#29 Loss: 0.0111 Acc: 100.0000% Time: 147.1675s\n",
            "#30 Loss: 0.2283 Acc: 90.0000% Time: 152.0315s\n",
            "#31 Loss: 0.0477 Acc: 100.0000% Time: 156.9289s\n",
            "#32 Loss: 0.1813 Acc: 90.0000% Time: 161.7721s\n",
            "#33 Loss: 0.3220 Acc: 90.0000% Time: 166.6919s\n",
            "#34 Loss: 0.0962 Acc: 100.0000% Time: 171.6078s\n",
            "#35 Loss: 0.0075 Acc: 100.0000% Time: 176.4663s\n",
            "#36 Loss: 0.2442 Acc: 90.0000% Time: 181.3270s\n",
            "#37 Loss: 0.6412 Acc: 80.0000% Time: 186.2170s\n",
            "#38 Loss: 0.0022 Acc: 100.0000% Time: 191.0799s\n",
            "#39 Loss: 0.4479 Acc: 90.0000% Time: 195.9810s\n",
            "#40 Loss: 0.3607 Acc: 80.0000% Time: 200.9477s\n",
            "#41 Loss: 0.0191 Acc: 100.0000% Time: 205.8919s\n",
            "#42 Loss: 0.0745 Acc: 100.0000% Time: 210.8113s\n",
            "#43 Loss: 0.1126 Acc: 90.0000% Time: 215.7448s\n",
            "#44 Loss: 0.8266 Acc: 80.0000% Time: 220.6674s\n",
            "#45 Loss: 0.1550 Acc: 90.0000% Time: 225.6647s\n",
            "#46 Loss: 0.0817 Acc: 100.0000% Time: 230.6067s\n",
            "#47 Loss: 0.2617 Acc: 90.0000% Time: 235.6517s\n",
            "#48 Loss: 0.0441 Acc: 100.0000% Time: 240.5214s\n",
            "#49 Loss: 0.3594 Acc: 90.0000% Time: 245.3978s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gPPAmPnk_Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb25afb-c123-49b6-ffd7-305f66f8b0c1"
      },
      "source": [
        "model.eval()\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "        imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "    epoch_loss = running_loss / len(test_datasets)\n",
        "    epoch_acc = running_corrects / len(test_datasets) * 100.\n",
        "    print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os4YudlBY47G"
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "imgName = ''\n",
        "\n",
        "for file in os.listdir(querys[0]):\n",
        "    for i in range(0, len(imgext), 1):\n",
        "        if imgext[i] in file.lower():\n",
        "            imgName = file\n",
        "            break\n",
        "\n",
        "image = Image.open(querys[0] + '/' + imgName)\n",
        "image = transforms_test(image).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    imshow(image.cpu().data[0], title='예측 결과: ' + class_names[preds[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLFqO9cpSmFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cdb0454-2901-4c1e-f469-e662e59ee17c"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlqhcM1ZXcro"
      },
      "source": [
        "import io\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, jsonify, request\n",
        "\n",
        "\n",
        "def get_prediction(image_bytes):\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "    image = transforms_test(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        imshow(image.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "    return class_names[preds[0]]\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "@app.route('/', methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        file = request.files['file']\n",
        "        image_bytes = file.read()\n",
        "\n",
        "        class_name = get_prediction(image_bytes=image_bytes)\n",
        "        print(\"결과:\", {'class_name': class_name})\n",
        "        return jsonify({'class_name': class_name})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMaDTEBRs_SS"
      },
      "source": [
        "* 클라이언트 테스트 방법\n",
        "\n",
        "<pre>\n",
        "curl -X POST -F file=@{이미지 파일명} {Ngrok 서버 주소}\n",
        "</pre>\n",
        "\n",
        "* 사용 예시\n",
        "\n",
        "<pre>\n",
        "curl -X POST -F file=@dongseok.jpg http://c4cdb8de3a35.ngrok.io/\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzrf6fbDaARx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1540e9be-b3f8-4865-8d97-7598f05d7b60"
      },
      "source": [
        "run_with_ngrok(app)\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://1ee53eb92e02.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [12/May/2021 00:40:02] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "결과: {'class_name': 'kotori'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [12/May/2021 00:40:06] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "결과: {'class_name': 'kotori'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [12/May/2021 00:40:12] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "결과: {'class_name': 'kotori'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [12/May/2021 00:40:16] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "결과: {'class_name': 'kotori'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}